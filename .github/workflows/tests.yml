name: Test Suite

on:
  workflow_call:
    # Can be called from other workflows
  workflow_dispatch:
    # Can be manually triggered
  pull_request:
    paths:
      - "packages/**"
      - "tests/**"
      - "package.json"
      - "package-lock.json"
      - "vitest.config.ts"
      - "playwright.config.ts"
  push:
    branches:
      - main
    paths:
      - "packages/**"
      - "tests/**"
      - "package.json"
      - "package-lock.json"
      - "vitest.config.ts"
      - "playwright.config.ts"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Note: Permissions are inherited from the calling workflow (ci.yml)
# Reusable workflows should not define their own permissions

env:
  NODE_NO_WARNINGS: 1
  NODE_ENV: test
  CI: true

jobs:
  # Job 1: Unit Tests (fastest, runs first)
  # Note: Typecheck and build are handled by ci.yml before this workflow runs
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    strategy:
      fail-fast: false
      matrix:
        test-path:
          - "tests/unit/main"
          - "tests/unit/renderer"
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22.x"
          cache: "npm"

      - name: Cache node_modules
        uses: actions/cache@v4
        with:
          path: |
            node_modules
            packages/*/node_modules
          key: node-deps-${{ runner.os }}-node22-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            node-deps-${{ runner.os }}-node22-

      - name: Install dependencies
        run: npm ci --prefer-offline --no-audit --no-fund --ignore-scripts
        env:
          ELECTRON_SKIP_BINARY_DOWNLOAD: 1
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: 1

      - name: Create test-results directory
        run: mkdir -p test-results

      - name: Set test path slug
        id: test-path-slug
        run: echo "slug=$(echo '${{ matrix.test-path }}' | tr '/' '-')" >> $GITHUB_OUTPUT

      - name: Check if test files exist
        id: check-tests
        run: |
          if [ -d "${{ matrix.test-path }}" ] && [ -n "$(find ${{ matrix.test-path }} -name '*.test.ts' -o -name '*.test.tsx' | head -1)" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "✓ Test files found in ${{ matrix.test-path }}"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "⚠ No test files found in ${{ matrix.test-path }}"
          fi

      - name: Run unit tests
        if: steps.check-tests.outputs.exists == 'true'
        run: npx vitest run ${{ matrix.test-path }} --reporter=verbose --reporter=junit --outputFile=test-results/unit-${{ steps.test-path-slug.outputs.slug }}.xml

      - name: Upload unit test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results-${{ steps.test-path-slug.outputs.slug }}
          path: test-results/unit-*.xml
          retention-days: 7
          if-no-files-found: ignore

  # Job 3: Component Tests
  component-tests:
    name: Component Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22.x"
          cache: "npm"

      - name: Cache node_modules
        uses: actions/cache@v4
        with:
          path: |
            node_modules
            packages/*/node_modules
          key: node-deps-${{ runner.os }}-node22-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            node-deps-${{ runner.os }}-node22-

      - name: Install dependencies
        run: npm ci --prefer-offline --no-audit --no-fund --ignore-scripts
        env:
          ELECTRON_SKIP_BINARY_DOWNLOAD: 1
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: 1

      - name: Create test-results directory
        run: mkdir -p test-results

      - name: Check if component test files exist
        id: check-tests
        run: |
          if [ -d "tests/components" ] && [ -n "$(find tests/components -name '*.test.ts' -o -name '*.test.tsx' | head -1)" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "✓ Component test files found"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "⚠ No component test files found"
          fi

      - name: Run component tests
        if: steps.check-tests.outputs.exists == 'true'
        run: npx vitest run tests/components --reporter=verbose --reporter=junit --outputFile=test-results/component-tests.xml

      - name: Upload component test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: component-test-results
          path: test-results/component-tests.xml
          retention-days: 7
          if-no-files-found: ignore

  # Job 4: Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    strategy:
      fail-fast: false
      matrix:
        test-path:
          - "tests/integration/main"
          - "tests/integration/renderer"
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22.x"
          cache: "npm"

      - name: Cache node_modules
        uses: actions/cache@v4
        with:
          path: |
            node_modules
            packages/*/node_modules
          key: node-deps-${{ runner.os }}-node22-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            node-deps-${{ runner.os }}-node22-

      - name: Install dependencies
        run: npm ci --prefer-offline --no-audit --no-fund --ignore-scripts
        env:
          ELECTRON_SKIP_BINARY_DOWNLOAD: 1
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: 1

      - name: Create test-results directory
        run: mkdir -p test-results

      - name: Set test path slug
        id: test-path-slug
        run: echo "slug=$(echo '${{ matrix.test-path }}' | tr '/' '-')" >> $GITHUB_OUTPUT

      - name: Check if test files exist
        id: check-tests
        run: |
          if [ -d "${{ matrix.test-path }}" ] && [ -n "$(find ${{ matrix.test-path }} -name '*.test.ts' -o -name '*.test.tsx' | head -1)" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "✓ Test files found in ${{ matrix.test-path }}"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "⚠ No test files found in ${{ matrix.test-path }}"
          fi

      - name: Run integration tests
        if: steps.check-tests.outputs.exists == 'true'
        run: npx vitest run ${{ matrix.test-path }} --reporter=verbose --reporter=junit --outputFile=test-results/integration-${{ steps.test-path-slug.outputs.slug }}.xml

      - name: Upload integration test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results-${{ steps.test-path-slug.outputs.slug }}
          path: test-results/integration-*.xml
          retention-days: 7
          if-no-files-found: ignore

  # Job 5: Test Coverage (runs all vitest tests with coverage)
  coverage:
    name: Test Coverage
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [unit-tests, component-tests, integration-tests]
    if: success()
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22.x"
          cache: "npm"

      - name: Cache node_modules
        uses: actions/cache@v4
        with:
          path: |
            node_modules
            packages/*/node_modules
          key: node-deps-${{ runner.os }}-node22-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            node-deps-${{ runner.os }}-node22-

      - name: Install dependencies
        run: npm ci --prefer-offline --no-audit --no-fund --ignore-scripts
        env:
          ELECTRON_SKIP_BINARY_DOWNLOAD: 1
          PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD: 1

      - name: Run tests with coverage
        run: npm run test:coverage

      - name: Upload coverage reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: |
            coverage/
            coverage/lcov.info
          retention-days: 30

      - name: Publish coverage to PR
        if: github.event_name == 'pull_request' && always()
        run: |
          echo "## Coverage Report" >> $GITHUB_STEP_SUMMARY
          if [ -f coverage/lcov.info ]; then
            echo "Coverage report generated successfully." >> $GITHUB_STEP_SUMMARY
            echo "View detailed coverage in the artifacts." >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Coverage report not found." >> $GITHUB_STEP_SUMMARY
          fi

  # Job 6: E2E Tests (requires build, runs on Windows for Electron compatibility)
  e2e-tests:
    name: E2E Tests
    runs-on: windows-2022
    timeout-minutes: 30
    env:
      PYTHON: python
      MSVS-VERSION: 2022
      GYP-MSVS-VERSION: 2022
      CI: true
      NODE_ENV: test
      ELECTRON_DISABLE_GPU: 1
      ELECTRON_NO_SANDBOX: 1
      ELECTRON_ENABLE_LOGGING: 1
      HARDWARE_SIMULATION_MODE: true
      MOCK_PRINTER_ENABLED: true
      MOCK_CARD_READER_ENABLED: true
      MOCK_SCANNER_ENABLED: true
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22.x"
          cache: "npm"

      - name: Setup MSBuild
        uses: microsoft/setup-msbuild@v2

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Cache node_modules
        uses: actions/cache@v4
        with:
          path: |
            node_modules
            packages/*/node_modules
            **/node_modules/better-sqlite3/build
            **/node_modules/node-hid/build
            **/node_modules/serialport/build
            **/node_modules/usb/build
          key: node-deps-${{ runner.os }}-node22-electron-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            node-deps-${{ runner.os }}-node22-electron-
            node-deps-${{ runner.os }}-node22-
            node-deps-${{ runner.os }}-node22-

      - name: Install dependencies
        run: npm ci --prefer-offline --no-audit --no-fund --ignore-scripts
        shell: powershell
        # Note: Don't skip Playwright browser download for E2E tests

      - name: Rebuild native modules
        run: npx electron-rebuild --force --only=better-sqlite3,node-hid,serialport,usb
        shell: powershell
        env:
          npm_config_build_from_source: true

      - name: Install Playwright browsers
        run: npx playwright install --with-deps
        shell: powershell

      - name: Verify Electron installation
        run: |
          Write-Host "Verifying Electron installation..."
          if (Test-Path "node_modules/electron/dist/electron.exe") {
            Write-Host "[OK] Electron binary found"
            npx electron --version
          } else {
            Write-Host "[ERROR] Electron binary not found, reinstalling..."
            # Clean electron installation
            if (Test-Path "node_modules/electron") {
              Remove-Item -Path "node_modules/electron" -Recurse -Force -ErrorAction SilentlyContinue
            }
            # Force reinstall electron
            npm install electron --force --no-save
            # Verify again
            if (Test-Path "node_modules/electron/dist/electron.exe") {
              Write-Host "[OK] Electron reinstalled successfully"
              npx electron --version
            } else {
              Write-Host "[ERROR] Electron still not found after reinstall"
              exit 1
            }
          }
        shell: powershell

      - name: Build application (minimal for E2E)
        run: npm run build
        shell: powershell
        env:
          NODE_ENV: production
          VITE_DISTRIBUTION_CHANNEL: test

      - name: Check if E2E test files exist
        id: check-e2e-tests
        run: |
          if (Test-Path "tests/e2e") {
            $testFiles = Get-ChildItem -Path tests/e2e -Filter "*.spec.ts" -Recurse
            if ($testFiles.Count -gt 0) {
              echo "exists=true" >> $env:GITHUB_OUTPUT
              Write-Host "\u2713 E2E test files found: $($testFiles.Count)"
            } else {
              echo "exists=false" >> $env:GITHUB_OUTPUT
              Write-Host "\u26a0 No E2E test files found"
            }
          } else {
            echo "exists=false" >> $env:GITHUB_OUTPUT
            Write-Host "\u26a0 tests/e2e directory not found"
          }
        shell: powershell

      - name: Run E2E tests
        if: steps.check-e2e-tests.outputs.exists == 'true'
        run: npm run test:e2e
        shell: powershell

      - name: Upload E2E test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-test-results
          path: |
            test-results/
            playwright-report/
          retention-days: 7

      - name: Upload E2E screenshots
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-screenshots
          path: test-results/**/*.png
          retention-days: 7

      - name: Upload E2E videos
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-videos
          path: test-results/**/*.webm
          retention-days: 7

  # Job 7: Test Summary (aggregates all test results)
  # Note: Typecheck is handled in ci.yml before this workflow
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, component-tests, integration-tests, coverage, e2e-tests]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          path: all-test-results
          pattern: "*test-results*"
          merge-multiple: true

      - name: Generate test summary
        run: |
          echo "# Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Execution Status" >> $GITHUB_STEP_SUMMARY
          echo "_Note: Type checking and build validation are performed in the main CI workflow before tests_" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Type | Status | Result |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|--------|" >> $GITHUB_STEP_SUMMARY

          # Unit tests
          if [ "${{ needs.unit-tests.result }}" == "success" ]; then
            echo "| Unit Tests | \u2705 Passed | ${{ needs.unit-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Unit Tests | \u274c Failed | ${{ needs.unit-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          # Component tests
          if [ "${{ needs.component-tests.result }}" == "success" ]; then
            echo "| Component Tests | \u2705 Passed | ${{ needs.component-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Component Tests | \u274c Failed | ${{ needs.component-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          # Integration tests
          if [ "${{ needs.integration-tests.result }}" == "success" ]; then
            echo "| Integration Tests | \u2705 Passed | ${{ needs.integration-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Integration Tests | \u274c Failed | ${{ needs.integration-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          # Coverage
          if [ "${{ needs.coverage.result }}" == "success" ]; then
            echo "| Coverage | \u2705 Passed | ${{ needs.coverage.result }} |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.coverage.result }}" == "skipped" ]; then
            echo "| Coverage | \u23ed Skipped | ${{ needs.coverage.result }} |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Coverage | \u26a0\ufe0f Warning | ${{ needs.coverage.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          # E2E tests
          if [ "${{ needs.e2e-tests.result }}" == "success" ]; then
            echo "| E2E Tests | \u2705 Passed | ${{ needs.e2e-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| E2E Tests | \u274c Failed | ${{ needs.e2e-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Test results, coverage reports, and screenshots are available in the workflow artifacts." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Count test result files if available
          if [ -d "all-test-results" ]; then
            TEST_COUNT=$(find all-test-results -name "*.xml" -type f 2>/dev/null | wc -l | tr -d ' ')
            echo "- Test result files: $TEST_COUNT" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Check if all tests passed
        run: |
          echo "Checking test results..."
          echo "Note: Type checking and build validation are performed in the main CI workflow before tests"
          echo ""

          # Check unit tests
          if [ "${{ needs.unit-tests.result }}" != "success" ]; then
            echo "❌ Unit tests failed"
            exit 1
          fi
          echo "✓ Unit tests passed"

          # Check component tests
          if [ "${{ needs.component-tests.result }}" != "success" ]; then
            echo "❌ Component tests failed"
            exit 1
          fi
          echo "✓ Component tests passed"

          # Check integration tests
          if [ "${{ needs.integration-tests.result }}" != "success" ]; then
            echo "❌ Integration tests failed"
            exit 1
          fi
          echo "✓ Integration tests passed"

          # Check E2E tests
          if [ "${{ needs.e2e-tests.result }}" != "success" ]; then
            echo "❌ E2E tests failed"
            exit 1
          fi
          echo "✓ E2E tests passed"

          # Coverage is optional, so we only warn
          if [ "${{ needs.coverage.result }}" != "success" ] && [ "${{ needs.coverage.result }}" != "skipped" ]; then
            echo "⚠️  Coverage generation had issues but not blocking"
          else
            echo "✓ Coverage passed"
          fi

          echo ""
          echo "✅ All tests passed successfully!"
